{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 21: Survival analysis \n",
    "\n",
    "UBC 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "# does lifelines try to mess with this?\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lifelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "\n",
    "- HW8 released, due November 29th 11:59pm. \n",
    "- Only one more homework left, which is going to be on communication, where you'll be writing a blog post. \n",
    "- Next lecture on Ethics is going to be a guest lecture by [Giulia Toti]( https://www.gtoti.com/), who is currently teaching [Computer and Society](https://www.cs.ubc.ca/course-section/cpsc-430-102-2022w) course and who has taught CPSC 330 before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "- Explain what is right-censored data. \n",
    "- Explain the problem with treating right-censored data the same as \"regular\" data.\n",
    "- Determine whether survival analysis is an appropriate tool for a given problem.\n",
    "- Apply survival analysis in Python using the `lifelines` package.\n",
    "- Interpret a survival curve, such as the Kaplan-Meier curve.\n",
    "- Interpret the coefficients of a fitted Cox proportional hazards model.\n",
    "- Make predictions for existing individuals and interpret these predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Customer churn: our standard approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imagine that you are working for a subscription-based telecom company. \n",
    "- You want to come up with retention strategies for different customer segments. \n",
    "- So you want to model the \"time to churn\" to understand different factors affecting customer churn.   \n",
    "- Suppose you are given this dataset [Customer Churn Dataset](https://www.kaggle.com/blastchar/telco-customer-churn), which is collected at a fixed time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "train_df, test_df = train_test_split(df, random_state=123)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are interested in predicting customer churn: the \"Churn\" column.  \n",
    "- How will you approach this problem with the approaches we have seen so far? \n",
    "- How about treating this as a binary classification problem where we want to predict `Churn` (yes/no) from these -other columns.\n",
    "- Before we look into survival analysis, let's just treat it as a binary classification model where we want to predict whether a customer churned or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Does this mean there is no missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ok, let's try our usual approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"SeniorCitizen\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "drop_features = [\"customerID\"]\n",
    "passthrough_features = [\"SeniorCitizen\"]\n",
    "target_column = [\"Churn\"]\n",
    "# the rest are categorical\n",
    "categorical_features = list(\n",
    "    set(train_df.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(passthrough_features)\n",
    "    - set(drop_features)\n",
    "    - set(target_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OneHotEncoder(), categorical_features),\n",
    "    (\"passthrough\", passthrough_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(train_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hmmm, one of the numeric features is causing problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, looks like `TotalCharges` is not a numeric type. What if we change the type of this column to float? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "train_df[\"TotalCharges\"] = train_df[\"TotalCharges\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Argh!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for val in train_df[\"TotalCharges\"]:\n",
    "    try:\n",
    "        float(val)\n",
    "    except ValueError:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well, it turns out we can't see those problematic values because they are whitespace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in train_df[\"TotalCharges\"]:\n",
    "    try:\n",
    "        float(val)\n",
    "    except ValueError:\n",
    "        print('\"%s\"' % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's replace the whitespaces with NaNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(\n",
    "    TotalCharges=train_df[\"TotalCharges\"].replace(\" \", np.nan).astype(float)\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    TotalCharges=test_df[\"TotalCharges\"].replace(\" \", np.nan).astype(float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "But now we are going to have missing values and we need to include imputation for numeric features in our preprocessor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler()),\n",
    "        numeric_features,\n",
    "    ),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"passthrough\", passthrough_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try that again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(train_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It worked! Let's get the column names of the transformed data from the column transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = (\n",
    "    numeric_features\n",
    "    + preprocessor.named_transformers_[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    "    .tolist()\n",
    "    + passthrough_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    ")\n",
    "X_test_enc = pd.DataFrame(\n",
    "    preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"Churn\"])\n",
    "X_test = test_df.drop(columns=[\"Churn\"])\n",
    "\n",
    "y_train = train_df[\"Churn\"]\n",
    "y_test = test_df[\"Churn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    dc, X_train, y_train, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy model scores are pretty good because we have class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"logistic regression\"] = mean_std_cross_val_scores(\n",
    "    lr, X_train, y_train, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, cross_val_predict(lr, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression beats the dummy model. \n",
    "- But it seems like we have many false negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RandomForestClassifier\n",
    "\n",
    "Let's try random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = make_pipeline(preprocessor, RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"random forest\"] = mean_std_cross_val_scores(\n",
    "    rf, X_train, y_train, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, cross_val_predict(rf, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Random forest is not improving the scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We might decide to do hyperparamter optimization to further improve the score. \n",
    "- But after trying out all the usual things should we be happy with the scores?\n",
    "- Are we doing anything fundamentally wrong when we treat this problem as a binary classification? \n",
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the class is about what is wrong with what we just did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Censoring and survival analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Time to event and censoring\n",
    "\n",
    "- When we treat the problem as a binary classification problem, we predict whether a customer would churn or not at a particular point in time, when the data was collected. \n",
    "- If a customer has not churned yet, wouldn't it be more useful to understand when they are likely to churn so that we can offer them promotions etc?  \n",
    "- Here we are actually interested in the time till the event of churn occurs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many situations where you want to analyze **the time until an event occurs**. For example,\n",
    "\n",
    "- the time until a customer leaves a subscription service (this dataset)\n",
    "- the time until a disease kills its host\n",
    "- the time until a piece of equipment breaks\n",
    "- the time that someone unemployed will take to land a new job\n",
    "- the time until you wait for your turn to get a surgery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this branch of statistics is usually referred to as **Survival Analysis**, the event in question does not need to be related to actual \"survival\". The important thing is to understand that we are interested in **the time until something happens**, or whether or not something will happen in a certain time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset there is a column called \"tenure\", which encodes this temporal aspect of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df[[\"tenure\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tenure column is the number of months the customer has stayed with the company. \n",
    "- But we only have information about this till the point we collected the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Question:** But why is this different? Can't you just use the techniques you learned so far (e.g., regression models) to predict the time (tenure in our case)? Take a minute to think about this.\n",
    "What could be possible scenarios for the duration column? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The answer would be yes if you could observe the actual time in all occurrences, but you usually cannot. Frequently, there will be some kind of **censoring** which will not allow you to observe the exact time that the event happened for all units/individuals that are being studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"tenure\", \"Churn\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What this means is that we **don't have correct target values** to train or test our model.\n",
    "- This is a problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider some approaches to deal with this censoring issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 1: Only consider the examples where \"Churn\"=Yes\n",
    "\n",
    "Let's just consider the cases _for which we have the time_, to obtain the average subscription length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_churn = train_df.query(\n",
    "    \"Churn == 'Yes'\"\n",
    ")  # Consider only examples where the customers churned.\n",
    "test_df_churn = test_df.query(\n",
    "    \"Churn == 'Yes'\"\n",
    ")  # Consider only examples where the customers churned.\n",
    "train_df_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_notenure = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler()),\n",
    "        numeric_features[1:],  # Getting rid of the tenure column\n",
    "    ),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"passthrough\", passthrough_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tenure_lm = make_pipeline(preprocessing_notenure, Ridge())\n",
    "\n",
    "tenure_lm.fit(train_df_churn.drop(columns=[\"tenure\"]), train_df_churn[\"tenure\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    tenure_lm.predict(test_df_churn.drop(columns=[\"tenure\"]))[:10],\n",
    "    columns=[\"tenure_predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will be wrong with our estimated survival times? Will they be too low or too high? \n",
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On average they will be **underestimates** (too small), because we are ignoring the currently subscribed (un-churned) customers. Our dataset is a biased sample of those who churned within the time window of the data collection. Long-time subscribers were more likely to be removed from the dataset! This is a common mistake - see the [Calling Bullshit video](https://www.youtube.com/watch?v=ITWQ5psx9Sw) from the README!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 2: Assume everyone churns right now\n",
    "\n",
    "Assume everyone churns right now - in other words, use the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"tenure\", \"Churn\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_lm.fit(train_df.drop(columns=[\"tenure\"]), train_df[\"tenure\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    tenure_lm.predict(test_df_churn.drop(columns=[\"tenure\"]))[:10],\n",
    "    columns=[\"tenure_predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will be wrong with our estimated survival time?\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df[[\"tenure\", \"Churn\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be an **underestimate** again. For those still subscribed, while we did not remove them, we recorded a total tenure shorter than in reality, because they will keep going for some amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 3: Survival analysis\n",
    "\n",
    "Deal with this properly using [survival analysis](https://en.wikipedia.org/wiki/Survival_analysis).\n",
    "\n",
    "- You may learn about this in a statistics course.\n",
    "- We will use the `lifelines` package in Python and will not go into the math/stats of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"tenure\", \"Churn\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Types of questions we might want to answer:\n",
    "\n",
    "1. How long do customers stay with the service? \n",
    "2. For a particular customer, can we predict how long they might stay with the service?\n",
    "3. What factors influence a customer's churn time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](img/eva-coffee.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaplan-Meier survival curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything further, I want to modify our dataset slightly:\n",
    "\n",
    "1. I'm going to drop the `TotalCharges` (yes, after all that work fixing it) because it's a bit of a strange feature.\n",
    "  - Its value actually changes over time, but we only have the value at the end.\n",
    "  - We still have `MonthlyCharges`.\n",
    "2. I'm going to not scale the `tenure` column, since it will be convenient to keep it in its original units of months. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just for our sanity, I'm redefining the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"MonthlyCharges\"]\n",
    "drop_features = [\"customerID\", \"TotalCharges\"]\n",
    "passthrough_features = [\"tenure\", \"SeniorCitizen\"]  # don't want to scale tenure\n",
    "target_column = [\"Churn\"]\n",
    "# the rest are categorical\n",
    "categorical_features = list(\n",
    "    set(train_df.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(passthrough_features)\n",
    "    - set(drop_features)\n",
    "    - set(target_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_final = make_column_transformer(\n",
    "    (\n",
    "        FunctionTransformer(lambda x: x == \"Yes\"),\n",
    "        target_column,\n",
    "    ),  # because we need it in this format for lifelines package\n",
    "    (\"passthrough\", passthrough_features),\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse=False), categorical_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_final.fit(train_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get the column names of the columns created by our column transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "new_columns = (\n",
    "    target_column\n",
    "    + passthrough_features\n",
    "    + numeric_features\n",
    "    + preprocessing_final.named_transformers_[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_surv = pd.DataFrame(\n",
    "    preprocessing_final.transform(train_df), index=train_df.index, columns=new_columns\n",
    ")\n",
    "test_df_surv = pd.DataFrame(\n",
    "    preprocessing_final.transform(test_df), index=test_df.index, columns=new_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df_surv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll start with a model called `KaplanMeierFitter` from `lifelines` package to get a Kaplan Meier curve.  \n",
    "- For this model we only use two columns: tenure and churn. \n",
    "- We do not use any other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = lifelines.KaplanMeierFitter()\n",
    "kmf.fit(train_df_surv[\"tenure\"], train_df_surv[\"Churn\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "kmf.survival_function_.plot();\n",
    "plt.title(\"Survival function of customer churn\")\n",
    "plt.xlabel(\"Time with service (months)\")\n",
    "plt.ylabel(\"Survival probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is this plot telling us?\n",
    "- It shows the probability of survival over time.\n",
    "- For example, after 20 months the probability of survival is ~0.8. \n",
    "- Over time it's going down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What's the average tenure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(train_df_surv[\"tenure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What's the average tenure of the people who churned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(train_df_surv.query(\"Churn == 1.0\")[\"tenure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What's the average tenure of the people who did not churn? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(train_df_surv.query(\"Churn == 0.0\")[\"tenure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's look at the histogram of number of people who have not churned. \n",
    "- The key point here is that people _joined at different times_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "train_df_surv[train_df_surv['Churn'] == 0][\"tenure\"].hist(grid=False)\n",
    "plt.xlabel(\"months\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the data was collected at a fixed time and these are the people who hadn't yet churned, those with larger `tenure` values here must have joined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lifelines can also give us some \"error bars\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf.plot()\n",
    "plt.title(\"Survival function of customer churn\")\n",
    "plt.xlabel(\"Time with service (months)\")\n",
    "plt.ylabel(\"Survival probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We already have some actionable information here.\n",
    "- The curve drops down fast at the beginning suggesting that people tend to leave early on. \n",
    "- If there would have been a big drop in the curve, it means a bunch of people left at that time (e.g., after a 1-month free trial). \n",
    "- BTW, the [original paper by Kaplan and Meier](https://web.stanford.edu/~lutian/coursepdf/KMpaper.pdf) has been cited over 57000 times!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also create the K-M curve for different subgroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = train_df_surv[\"tenure\"]\n",
    "E = train_df_surv[\"Churn\"]\n",
    "senior = train_df_surv[\"SeniorCitizen\"] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "\n",
    "kmf.fit(T[senior], event_observed=E[senior], label=\"Senior Citizens\")\n",
    "kmf.plot(ax=ax)\n",
    "\n",
    "kmf.fit(T[~senior], event_observed=E[~senior], label=\"Non-Senior Citizens\")\n",
    "kmf.plot(ax=ax)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Time with service (months)\")\n",
    "plt.ylabel(\"Survival probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like senior citizens churn more quickly than others.\n",
    "- This is quite useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cox proportional hazards model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We haven't been incorporating other features in the model so far. \n",
    "- The Cox proportional hazards model is a commonly used model that allows us to interpret how features influence a censored tenure/duration. \n",
    "- You can think of it like linear regression for survival analysis: we will get a coefficient for each feature that tells us how it influences survival.\n",
    "- It makes some strong assumptions (the proportional hazards assumption) that may not be true, but we won't go into this here.\n",
    "- The proportional hazard model works multiplicatively, like linear regression with log-transformed targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "cph = lifelines.CoxPHFitter()\n",
    "# cph.fit(train_df_surv, duration_col=\"tenure\", event_col=\"Churn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ok, going to [this URL](https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model), it seems the easiest solution is to add a penalizer.\n",
    "  - FYI this is related to switching from `LinearRegression` to `Ridge`.\n",
    "  - Adding `drop='first'` on our OHE might have helped with this.\n",
    "  - (For 340 folks: we're adding regularization; `lifelines` adds both L1 and L2 regularization, aka elastic net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cph = lifelines.CoxPHFitter(penalizer=0.1)\n",
    "cph.fit(train_df_surv, duration_col=\"tenure\", event_col=\"Churn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can look at the coefficients learned by the model and start interpreting them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cph_params = pd.DataFrame(cph.params_).sort_values(by=\"coef\", ascending=False)\n",
    "cph_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like month-to-month leads to more churn, two-year contract leads to less churn; this makes sense!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# cph.baseline_hazard_ # baseline hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cph.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Could we have gotten this type of information out of sklearn?\n",
    "- Yes, but the above approach is more appropriate for such problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train.drop(columns=[\"tenure\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I'm redefining feature types and our preprocessor for our sanity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"MonthlyCharges\"]\n",
    "drop_features = [\"customerID\", \"tenure\", \"TotalCharges\"]\n",
    "passthrough_features = [\"SeniorCitizen\"]\n",
    "target_column = [\"Churn\"]\n",
    "# the rest are categorical\n",
    "categorical_features = list(\n",
    "    set(train_df.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(passthrough_features)\n",
    "    - set(drop_features)\n",
    "    - set(target_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler()),\n",
    "        numeric_features,\n",
    "    ),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"passthrough\", passthrough_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "new_columns = (\n",
    "    numeric_features\n",
    "    + preprocessor.named_transformers_[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    "    .tolist()\n",
    "    + passthrough_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "lr.fit(X_train, y_train)\n",
    "lr_coefs = pd.DataFrame(\n",
    "    data=np.squeeze(lr[1].coef_), index=new_columns, columns=[\"Coefficient\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.sort_values(by=\"Coefficient\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is some agreement, which is good.\n",
    "- But our survival model is much more useful.\n",
    "  - Not to mention more correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One thing we get with `lifelines` is confidence intervals on the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "cph.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- (We could probably get the same for logistic regression if using `statsmodels` instead of sklearn.)\n",
    "- However, in general, I would be careful with all of this.\n",
    "- Ideally we would have more statistical training when using `lifelines` - there is a lot that can go wrong.\n",
    "  - It comes with various diagnostics as well.\n",
    "- But I think it's very useful to know about survival analysis and the availability of software to deal with it.\n",
    "- Oh, and there are lots of other nice plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's look at the survival plots for the people with \n",
    "    - two-year contract (Contract_Two year = 1) and \n",
    "    - people without two-year contract (Contract_Two year = 0)\n",
    "- As expected, the former survive longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.plot_partial_effects_on_outcome(\"Contract_Two year\", [0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's look at the survival plots for the people with different MonthlyCharges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.plot_partial_effects_on_outcome(\"MonthlyCharges\", [10, 100, 1000, 10_000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- That's the thing with linear models, they can't stop the growth.\n",
    "- We have a negative coefficient associated with `MonthlyCharges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_params.loc[\"MonthlyCharges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your monthly charges are huge, it takes this to the extreme and thinks you'll basically never churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use survival analysis to make predictions as well.\n",
    "- Here is the expected number of months to churn for the first 5 customers in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_surv.drop(columns=[\"tenure\", \"Churn\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_df_surv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How long each non-churned customer is likely to stay according to the model assuming that they just joined right now?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cph.predict_expectation(test_df_surv).head()  # assumes they just joined right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Survival curves for first 5 customers in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.predict_survival_function(test_df_surv[:5]).plot()\n",
    "plt.xlabel(\"Time with service (months)\")\n",
    "plt.ylabel(\"Survival probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `predict_survival_function` documentation:\n",
    "\n",
    "> Predict the survival function for individuals, given their covariates. This assumes that the individual just entered the study (that is, we do not condition on how long they have already lived for.) \n",
    "\n",
    "So these curves are \"starting now\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There's no probability prerequisite for this course, so this is optional material.\n",
    "- But you can do some interesting stuff here with conditional probabilities.\n",
    "- \"Given that a customer has been here 5 months, what's the outlook?\"\n",
    "  - It will be different than for a new customer. \n",
    "  - Thus, we might still want to predict for the non-churned customers in the training set!\n",
    "  - Not something we really thought about with our traditional supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get the customers who have not churned yet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df_surv_not_churned = train_df_surv[train_df_surv[\"Churn\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can _condition_ on the person having been around for 20 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.predict_survival_function(train_df_surv_not_churned[:1], conditional_after=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cph.predict_survival_function(train_df_surv_not_churned[:1]).plot(ax=plt.gca())\n",
    "preds = cph.predict_survival_function(\n",
    "    train_df_surv_not_churned[:1], conditional_after=20\n",
    ")\n",
    "plt.plot(preds.index[20:], preds.values[:-20])\n",
    "plt.xlabel(\"Time with service (months)\")\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.legend([\"Starting now\", \"Given 20 more months of service\"])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([1, 50]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at how the survival function (and expected lifetime) is much longer _given_ that the customer has already lasted 20 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How long each non-churned customer is likely to stay according to the model assuming that they have been here for the tenure time? \n",
    "- So, we can set this to their actual tenure so far to get a prediciton of what will happen going forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.predict_survival_function(\n",
    "    train_df_surv_not_churned[:1],\n",
    "    conditional_after=train_df_surv_not_churned[:1][\"tenure\"],\n",
    ").plot()\n",
    "plt.xlabel(\"Time into the future (months)\")\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Another useful application: you could ask what is the [customer lifetime value](https://en.wikipedia.org/wiki/Customer_lifetime_value).\n",
    "  - Basically, how much money do you expect to make off this customer between now and when they churn?\n",
    "- With regular supervised learning, tenure was a feature and we could only predict whether or not they had churned by then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation \n",
    "\n",
    "By default score returns \"partial log likelihood\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.score(train_df_surv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.score(test_df_surv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can look at the \"concordance index\" which is more interpretable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.concordance_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.score(train_df_surv, scoring_method=\"concordance_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.score(test_df_surv, scoring_method=\"concordance_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the documentation [here](https://lifelines.readthedocs.io/en/latest/Survival%20Regression.html#model-selection-and-calibration-in-survival-regression):\n",
    "\n",
    "> Another censoring-sensitive measure is the concordance-index, also known as the c-index. This measure evaluates the accuracy of the ranking of predicted time. It is in fact a generalization of AUC, another common loss function, and is interpreted similarly:\n",
    "> \n",
    "> - 0.5 is the expected result from random predictions,\n",
    "> - 1.0 is perfect concordance and,\n",
    "> - 0.0 is perfect anti-concordance (multiply predictions with -1 to get 1.0)\n",
    "> \n",
    "> [Here](https://stats.stackexchange.com/a/478305/11867) is an excellent introduction & description of the c-index for new users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cph.log_likelihood_ratio_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cph.check_assumptions(df_train_surv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other approaches / what did we not cover? (5 min)\n",
    "\n",
    "There are many other approaches to modelling in survival analysis:\n",
    "\n",
    "- Time-varying proportional hazards.\n",
    "  - What if some of the features change over time, e.g. plan type, number of lines, etc.\n",
    "- Approaches based on deep learning, e.g. the [pysurvival](https://square.github.io/pysurvival/) package.\n",
    "- Random survival forests.\n",
    "- And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of censoring\n",
    "There are also various types and sub-types of censoring we didn't cover:\n",
    "\n",
    "- What we did today is called \"right censoring\"\n",
    "- Sub-types within right censoring\n",
    "  - Did everyone join at the same time?\n",
    "  - Other reasons the data might be censored at random times, e.g. the person died?\n",
    "- Left censoring\n",
    "- Interval censoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Censoring and incorrect approaches to handling it\n",
    "  - Throw away people who haven't churned\n",
    "  - Assume everyone churns today\n",
    "- Predicting tenure vs. churned\n",
    "- Survival analysis encompasses both of these, and deals with censoring\n",
    "- And it can make rich and interesting predictions!\n",
    "- KM model -> doesn't look at features\n",
    "- CPH model -> like linear regression, does look at the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "Some people working with this same dataset:\n",
    "\n",
    "- https://medium.com/@zachary.james.angell/applying-survival-analysis-to-customer-churn-40b5a809b05a\n",
    "- https://towardsdatascience.com/churn-prediction-and-prevention-in-python-2d454e5fd9a5 (Cox)\n",
    "- https://towardsdatascience.com/survival-analysis-in-python-a-model-for-customer-churn-e737c5242822\n",
    "- https://towardsdatascience.com/survival-analysis-intuition-implementation-in-python-504fde4fcf8e\n",
    "\n",
    "lifelines documentation: \n",
    "- https://lifelines.readthedocs.io/en/latest/Survival%20analysis%20with%20lifelines.html\n",
    "- https://lifelines.readthedocs.io/en/latest/Survival%20Analysis%20intro.html#introduction-to-survival-analysis"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
